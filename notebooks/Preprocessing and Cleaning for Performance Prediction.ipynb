{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7a616e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import dump\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b402e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sklearn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sys\u001b[38;5;241m.\u001b[39mversion)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscikit-learn:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sklearn\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumPy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, numpy\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoblib:\u001b[39m\u001b[38;5;124m\"\u001b[39m, joblib\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sklearn' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Python:\", sys.version)\n",
    "print(\"scikit-learn:\", sklearn.__version__)\n",
    "print(\"NumPy:\", numpy.__version__)\n",
    "print(\"joblib:\", joblib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5a9222",
   "metadata": {},
   "source": [
    "# Preprocessing and Cleaning\n",
    "\n",
    "This section of the project builds on and is heavily influenced by the work done by **Biswajit Basak** (in particular the Preprocessing Pipelines) who also used this dataset.\n",
    "\n",
    "Link to the file used - https://github.com/thecuriousjuel/Machine-Learning-Project-iNeuron/blob/main/Jupyter%20Notebook%20File/ML%20Project%20Final.ipynb\n",
    "\n",
    "\n",
    "## Data Quality Assessment \n",
    "\n",
    "In order to influence our preproccessing and cleaning we need to undetstand the form our data is in and what needs to be changed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dc9457",
   "metadata": {},
   "source": [
    "## First we will deal with null values\n",
    "\n",
    "In particular we will try and identify rows which are majority null (over 30/42 attributes missing) and remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8907071e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "836\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "connection = sqlite3.connect('playerdata.sqlite')\n",
    "player_data = pd.read_sql_query(\"SELECT * FROM Player_Attributes\", connection)\n",
    "\n",
    "null_threshold = 21\n",
    "\n",
    "num_nulls_per_row = player_data.isnull().sum(axis=1)\n",
    "mostly_null_rows = num_nulls_per_row > null_threshold\n",
    "print(mostly_null_rows.sum())\n",
    "\n",
    "num_fully_null_rows = player_data.isnull().all(axis=1).sum()\n",
    "print(num_fully_null_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be551d72",
   "metadata": {},
   "source": [
    "### Insight\n",
    "\n",
    "After experimentiing with the null threshold realised that 836 rows were majority null all of them around 37 null entries and no rows had anything greater than that. Also there were 2319 rows with 8/42 null this i thought we could work with so i decided to set the threshold as greater than 9 null. This means, removing 836 records resulting in a better quality dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c08986fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183978\n",
      "183142\n"
     ]
    }
   ],
   "source": [
    "player_data_cleaned = player_data.copy()\n",
    "player_data_cleaned = player_data_cleaned.dropna(thresh=9)\n",
    "print(len(player_data))\n",
    "print(len(player_data_cleaned))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48abea3",
   "metadata": {},
   "source": [
    "With the remaining dataset still need to fix the remaining null vlaues so next i will show how many of the remaining attributes have null values and how many."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44305a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attacking_work_rate    2394\n",
      "volleys                1877\n",
      "curve                  1877\n",
      "agility                1877\n",
      "balance                1877\n",
      "jumping                1877\n",
      "vision                 1877\n",
      "sliding_tackle         1877\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the null values for each column\n",
    "null = player_data_cleaned.isnull().sum()\n",
    "filtered_null = null[null>0]\n",
    "# Print the percentage of null values for each column\n",
    "print(filtered_null.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efcf86a",
   "metadata": {},
   "source": [
    "We can see here that 8 attributes contain null values and 7 of them are continuous (volleys, curve, agility, balance, jumping, vision, sliding_tackle) and one is categorical attacking_work_rate\n",
    "\n",
    "**How we can imputate values for these null values?**\n",
    "\n",
    "For <u>numerical/continuous</u> the options are using the median or mean of the column, or even more sophisticated approaches like K-Nearest Neighbors (KNN) imputation, which considers the similarity between rows. **I will use mean**\n",
    "\n",
    "For <u>categorical</u> the most common approach is assigning mode value. **I will use mode**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c58fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['volleys', 'curve', 'agility', 'balance', 'jumping', 'vision', 'sliding_tackle']:\n",
    "    player_data_cleaned[column].fillna(player_data_cleaned[column].mean(), inplace=True)\n",
    "    \n",
    "mode_value = player_data_cleaned['attacking_work_rate'].mode()[0]\n",
    "player_data_cleaned['attacking_work_rate'].fillna(mode_value, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9db6d19",
   "metadata": {},
   "source": [
    "Now all the null values have been dealt with we can move onto the next part of the preprocessing stage "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cde60a9",
   "metadata": {},
   "source": [
    "## Feature Removal \n",
    "\n",
    " 1. In the next stage I am diverging greatly from Biswajit Basak's proposed method, I've opted to maintain multiple records for individual players, leveraging the variability in their performance across different matches rather than averaging attributes for every player. \n",
    "#### WHY? \n",
    "This approach captures the dynamic nature of football performance, influenced by factors such as match context, opponent strength, and player condition, providing a richer dataset for the model.\n",
    "\n",
    "2. Next steps include removing player IDs and match dates from the dataset. This step aims to minimize the focus on individual player identities, shifting the analysis towards a broader understanding of how various attributes impact overall performance in football."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e558b062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   overall_rating  potential preferred_foot attacking_work_rate  \\\n",
      "0            67.0       71.0          right              medium   \n",
      "1            67.0       71.0          right              medium   \n",
      "2            62.0       66.0          right              medium   \n",
      "3            61.0       65.0          right              medium   \n",
      "4            61.0       65.0          right              medium   \n",
      "\n",
      "  defensive_work_rate  crossing  finishing  heading_accuracy  short_passing  \\\n",
      "0              medium      49.0       44.0              71.0           61.0   \n",
      "1              medium      49.0       44.0              71.0           61.0   \n",
      "2              medium      49.0       44.0              71.0           61.0   \n",
      "3              medium      48.0       43.0              70.0           60.0   \n",
      "4              medium      48.0       43.0              70.0           60.0   \n",
      "\n",
      "   volleys  ...  vision  penalties  marking  standing_tackle  sliding_tackle  \\\n",
      "0     44.0  ...    54.0       48.0     65.0             69.0            69.0   \n",
      "1     44.0  ...    54.0       48.0     65.0             69.0            69.0   \n",
      "2     44.0  ...    54.0       48.0     65.0             66.0            69.0   \n",
      "3     43.0  ...    53.0       47.0     62.0             63.0            66.0   \n",
      "4     43.0  ...    53.0       47.0     62.0             63.0            66.0   \n",
      "\n",
      "   gk_diving  gk_handling  gk_kicking  gk_positioning  gk_reflexes  \n",
      "0        6.0         11.0        10.0             8.0          8.0  \n",
      "1        6.0         11.0        10.0             8.0          8.0  \n",
      "2        6.0         11.0        10.0             8.0          8.0  \n",
      "3        5.0         10.0         9.0             7.0          7.0  \n",
      "4        5.0         10.0         9.0             7.0          7.0  \n",
      "\n",
      "[5 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "player_data_cleaned = player_data_cleaned.drop(['player_fifa_api_id', 'player_api_id', 'id','date'], axis=1)\n",
    "print(player_data_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e65d389",
   "metadata": {},
   "source": [
    "## Further Cleaning\n",
    "\n",
    "Also after the initial data analysis, I decided to **drop** the 'curve' and 'potential' attributes from the dataset.\n",
    "\n",
    "1. **'Curve'**  as it is too specific and not universally applicable across all player roles\n",
    "2. **'Potential'** due to the speculative nature and not reflectoing current ability or performance. \n",
    "\n",
    "This decision allows for a more focused analysis on attributes directly observable and quantifiable in terms of their effect on player performance, potentially leading to a model that is more practical and grounded in real-world applicability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6c03719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   overall_rating preferred_foot attacking_work_rate defensive_work_rate  \\\n",
      "0            67.0          right              medium              medium   \n",
      "1            67.0          right              medium              medium   \n",
      "2            62.0          right              medium              medium   \n",
      "3            61.0          right              medium              medium   \n",
      "4            61.0          right              medium              medium   \n",
      "\n",
      "   crossing  finishing  heading_accuracy  short_passing  volleys  dribbling  \\\n",
      "0      49.0       44.0              71.0           61.0     44.0       51.0   \n",
      "1      49.0       44.0              71.0           61.0     44.0       51.0   \n",
      "2      49.0       44.0              71.0           61.0     44.0       51.0   \n",
      "3      48.0       43.0              70.0           60.0     43.0       50.0   \n",
      "4      48.0       43.0              70.0           60.0     43.0       50.0   \n",
      "\n",
      "   ...  vision  penalties  marking  standing_tackle  sliding_tackle  \\\n",
      "0  ...    54.0       48.0     65.0             69.0            69.0   \n",
      "1  ...    54.0       48.0     65.0             69.0            69.0   \n",
      "2  ...    54.0       48.0     65.0             66.0            69.0   \n",
      "3  ...    53.0       47.0     62.0             63.0            66.0   \n",
      "4  ...    53.0       47.0     62.0             63.0            66.0   \n",
      "\n",
      "   gk_diving  gk_handling  gk_kicking  gk_positioning  gk_reflexes  \n",
      "0        6.0         11.0        10.0             8.0          8.0  \n",
      "1        6.0         11.0        10.0             8.0          8.0  \n",
      "2        6.0         11.0        10.0             8.0          8.0  \n",
      "3        5.0         10.0         9.0             7.0          7.0  \n",
      "4        5.0         10.0         9.0             7.0          7.0  \n",
      "\n",
      "[5 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "player_data_cleaned = player_data_cleaned.drop(['curve', 'potential'], axis=1)\n",
    "print(player_data_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38baa41b",
   "metadata": {},
   "source": [
    "## Categorical Attribute cleaning\n",
    "\n",
    "Now that we have cleaned and preprocessed the Numerical Attributes we need to tackle the categorical ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3491a3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Cleaning\n",
      "['right' 'left']\n",
      "['medium' 'high' 'low' 'None' 'le' 'norm' 'stoc' 'y']\n",
      "['medium' 'high' 'low' '_0' '5' 'ean' 'o' '1' 'ormal' '7' '2' '8' '4'\n",
      " 'tocky' '0' '3' '6' '9' 'es'] \n",
      "\n",
      "After Cleaning\n",
      "['right' 'left']\n",
      "['medium' 'high' 'low']\n",
      "['medium' 'high' 'low'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Before Cleaning\")\n",
    "print(player_data_cleaned['preferred_foot'].unique())\n",
    "print(player_data_cleaned['attacking_work_rate'].unique())\n",
    "print(player_data_cleaned['defensive_work_rate'].unique(),\"\\n\")\n",
    "\n",
    "# Clean attacking_work_rate\n",
    "player_data_cleaned['attacking_work_rate'] = player_data_cleaned['attacking_work_rate'].replace({\n",
    "    'None': 'medium', 'le': 'low', 'norm': 'medium', 'stoc': 'medium', 'y': 'high'\n",
    "})\n",
    "\n",
    "# Standardize defensive_work_rate\n",
    "player_data_cleaned['defensive_work_rate'] = player_data_cleaned['defensive_work_rate'].apply(lambda x: 'low' if x in ['_0', '1', '2', '3', 'ean', 'o', 'tocky', '0', 'low'] else ('medium' if x in ['4', '5', '6', 'ormal', 'medium'] else ('high' if x in ['7', '8', '9', 'es','high'] else 'medium')))\n",
    "\n",
    "print(\"After Cleaning\")\n",
    "print(player_data_cleaned['preferred_foot'].unique())\n",
    "print(player_data_cleaned['attacking_work_rate'].unique())\n",
    "print(player_data_cleaned['defensive_work_rate'].unique(),\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36b6fea",
   "metadata": {},
   "source": [
    "Now that we have removed all the erraneous entries in our categorical features we can use OneHotEncoding to transform it into a form that can be fed into a ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8cb2a8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\test\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = ['preferred_foot', 'defensive_work_rate', 'attacking_work_rate']\n",
    "\n",
    "encoder = OneHotEncoder(sparse= False)\n",
    "\n",
    "encoded_data = encoder.fit_transform(player_data_cleaned[categorical_columns])\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "\n",
    "player_data_cleaned = player_data_cleaned.drop(columns=categorical_columns)\n",
    "player_data_cleaned = pd.concat([player_data_cleaned.reset_index(drop=True), encoded_df.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad6fcdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   overall_rating  crossing  finishing  heading_accuracy  short_passing  \\\n",
      "0            67.0      49.0       44.0              71.0           61.0   \n",
      "1            67.0      49.0       44.0              71.0           61.0   \n",
      "2            62.0      49.0       44.0              71.0           61.0   \n",
      "3            61.0      48.0       43.0              70.0           60.0   \n",
      "4            61.0      48.0       43.0              70.0           60.0   \n",
      "\n",
      "   volleys  dribbling  free_kick_accuracy  long_passing  ball_control  ...  \\\n",
      "0     44.0       51.0                39.0          64.0          49.0  ...   \n",
      "1     44.0       51.0                39.0          64.0          49.0  ...   \n",
      "2     44.0       51.0                39.0          64.0          49.0  ...   \n",
      "3     43.0       50.0                38.0          63.0          48.0  ...   \n",
      "4     43.0       50.0                38.0          63.0          48.0  ...   \n",
      "\n",
      "   gk_positioning  gk_reflexes  preferred_foot_left  preferred_foot_right  \\\n",
      "0             8.0          8.0                  0.0                   1.0   \n",
      "1             8.0          8.0                  0.0                   1.0   \n",
      "2             8.0          8.0                  0.0                   1.0   \n",
      "3             7.0          7.0                  0.0                   1.0   \n",
      "4             7.0          7.0                  0.0                   1.0   \n",
      "\n",
      "   defensive_work_rate_high  defensive_work_rate_low  \\\n",
      "0                       0.0                      0.0   \n",
      "1                       0.0                      0.0   \n",
      "2                       0.0                      0.0   \n",
      "3                       0.0                      0.0   \n",
      "4                       0.0                      0.0   \n",
      "\n",
      "   defensive_work_rate_medium  attacking_work_rate_high  \\\n",
      "0                         1.0                       0.0   \n",
      "1                         1.0                       0.0   \n",
      "2                         1.0                       0.0   \n",
      "3                         1.0                       0.0   \n",
      "4                         1.0                       0.0   \n",
      "\n",
      "   attacking_work_rate_low  attacking_work_rate_medium  \n",
      "0                      0.0                         1.0  \n",
      "1                      0.0                         1.0  \n",
      "2                      0.0                         1.0  \n",
      "3                      0.0                         1.0  \n",
      "4                      0.0                         1.0  \n",
      "\n",
      "[5 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "print(player_data_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a64ee55",
   "metadata": {},
   "source": [
    "## Standard Scaling\n",
    "\n",
    "Next step is Feature scaling. This can significantly impact the performance of models sensitive to input scales, such as gradient descent-based algorithms, k-nearest neighbors, and models using regularization. We only have to do this to the numerical data as the categorical is already OneHotEncoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2811c50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crossing              55.086883\n",
      "finishing             49.921078\n",
      "heading_accuracy      57.266023\n",
      "short_passing         62.429672\n",
      "volleys               49.468436\n",
      "dribbling             59.175154\n",
      "free_kick_accuracy    49.380950\n",
      "long_passing          57.069880\n",
      "ball_control          63.388879\n",
      "acceleration          67.659357\n",
      "sprint_speed          68.051244\n",
      "agility               65.970910\n",
      "reactions             66.103706\n",
      "balance               65.189496\n",
      "shot_power            61.808427\n",
      "jumping               66.969045\n",
      "stamina               67.038544\n",
      "strength              67.424529\n",
      "long_shots            53.339431\n",
      "aggression            60.948046\n",
      "interceptions         52.009271\n",
      "positioning           55.786504\n",
      "vision                57.873550\n",
      "penalties             55.003986\n",
      "marking               46.772242\n",
      "standing_tackle       50.351257\n",
      "sliding_tackle        48.001462\n",
      "gk_diving             14.704393\n",
      "gk_handling           16.063612\n",
      "gk_kicking            20.998362\n",
      "gk_positioning        16.132154\n",
      "gk_reflexes           16.441439\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "numerical_attributes = numerical_attributes = ['crossing', 'finishing', 'heading_accuracy', 'short_passing', 'volleys', 'dribbling', 'free_kick_accuracy', 'long_passing', 'ball_control', 'acceleration', 'sprint_speed', 'agility', 'reactions', 'balance', 'shot_power', 'jumping', 'stamina', 'strength', 'long_shots', 'aggression', 'interceptions', 'positioning', 'vision', 'penalties', 'marking', 'standing_tackle', 'sliding_tackle', 'gk_diving', 'gk_handling', 'gk_kicking', 'gk_positioning', 'gk_reflexes']\n",
    "\n",
    "numerical_data = player_data_cleaned[numerical_attributes]\n",
    "mean_values = numerical_data.mean()\n",
    "print(mean_values)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_numerical_data = scaler.fit_transform(numerical_data)\n",
    "scaled_numerical_df = pd.DataFrame(scaled_numerical_data, columns=numerical_attributes, index=numerical_data.index)\n",
    "\n",
    "player_data_cleaned = player_data_cleaned.drop(columns=numerical_attributes)\n",
    "\n",
    "player_data_cleaned_scaled = pd.concat([player_data_cleaned, scaled_numerical_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "748c496d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.joblib']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(encoder, 'encoder.joblib')\n",
    "dump(scaler, 'scaler.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16891280",
   "metadata": {},
   "source": [
    "## Test/Train split\n",
    "\n",
    "Now that our data has been cleaned and preprocessed we are nearly ready for model training. All we need to do is split the target attribute from the rest of the features and then split into test and train sets so we can accurately and fairly evaluate our model. \n",
    "\n",
    "We do this by first splitting the target and features then randomly picking 80% of the data to be in our training set and 20% to be in our test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98617fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = player_data_cleaned_scaled['overall_rating']\n",
    "features = player_data_cleaned_scaled.drop('overall_rating', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1c5d36",
   "metadata": {},
   "source": [
    "### Finally Convert to CSV for next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a306dc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('X_train.csv', index=False)\n",
    "X_test.to_csv('X_test.csv', index=False)\n",
    "y_train.to_csv('y_train.csv', index=False)\n",
    "y_test.to_csv('y_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc92159c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[59.59666667]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\test\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from joblib import load\n",
    "\n",
    "model = load('predictionmodel.joblib')\n",
    "test_prediction = model.predict([[0, 1, 0, 0, 1, 0, 0, 1, -0.005, 0.004, -0.016, -0.030, -0.025, -0.009, -0.021, -0.004, -0.025, 0.026, -0.799, 0.002, -0.011, -0.014, 0.011, 0.002, -0.002, -0.035, -0.018, 0.003, -0.0004, 0.011, 0.008, -0.0002, 0.010, -0.016, -0.00006, -0.041, -0.004, 0.00007, -0.008, -0.025]])\n",
    "print(test_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
